{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behind-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '../../../../'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "sys.path.append(ROOT_PATH)\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "import torchvision\n",
    "import glob\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.transforms as gnn_T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch_cluster import grid_cluster\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "from torch_geometric.data import Batch, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "random-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "split-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-interpretation",
   "metadata": {},
   "source": [
    "## Setup and Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "olive-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = './MNIST/graphs/'\n",
    "\n",
    "graph_train_dataset = MNISTSuperpixels(root=dataset_folder, train=True)\n",
    "graph_test_dataset = MNISTSuperpixels(root=dataset_folder, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flexible-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = gnn_T.Compose([\n",
    "    gnn_T.Cartesian(),\n",
    "    gnn_T.ToSparseTensor(remove_edge_index=False),\n",
    "    gnn_T.ToUndirected(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-moral",
   "metadata": {},
   "source": [
    "## Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hired-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplineCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SplineCNN, self).__init__()\n",
    "        self.conv1 = gnn.SplineConv(1, 32, dim=2, kernel_size=5)\n",
    "        self.conv2 = gnn.SplineConv(32, 64, dim=2, kernel_size=5)\n",
    "        self.fc1 = torch.nn.Linear(64, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        cluster = gnn.graclus(edge_index, num_nodes=x.size(0))\n",
    "        data = Batch(x=x, edge_index=edge_index, edge_attr=edge_attr, batch=batch)\n",
    "        data = gnn.max_pool(cluster, data)\n",
    "        \n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        x = F.elu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        cluster = gnn.graclus(edge_index, num_nodes=x.size(0))\n",
    "        data = Batch(x=x, edge_index=edge_index, edge_attr=edge_attr, batch=batch)\n",
    "        data = gnn.max_pool(cluster, data)\n",
    "        \n",
    "        x, batch = data.x, data.batch\n",
    "        \n",
    "        x = gnn.global_mean_pool(x, batch)  \n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "classical-medline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplineCNN(\n",
       "  (conv1): SplineConv(1, 32, dim=2)\n",
       "  (conv2): SplineConv(32, 64, dim=2)\n",
       "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SplineCNN\n",
    "model = SplineCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pending-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "train_frac = 0.8\n",
    "train_idx = int(len(graph_train_dataset) * train_frac)\n",
    "train_split = graph_train_dataset[:train_idx]\n",
    "val_split = graph_train_dataset[train_idx:]\n",
    "\n",
    "batch_size = 16\n",
    "graph_train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=False)\n",
    "graph_val_loader = DataLoader(val_split, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incomplete-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset_loader, preprocessor, device):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_dataset_loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = preprocessor(batch.to(device))\n",
    "        out = model(batch)\n",
    "        loss = F.nll_loss(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def validate_model(model, val_dataset_loader, preprocessor, device):\n",
    "    model.eval()\n",
    "    \n",
    "    val_acc = []\n",
    "    \n",
    "    for batch in val_dataset_loader:\n",
    "        batch = preprocessor(batch.to(device))\n",
    "        pred_logits = model(batch)\n",
    "        _, pred = pred_logits.max(dim=1)\n",
    "        acc = (pred.eq(batch.y).sum() / batch.y.size(0)).item()\n",
    "        val_acc.append(acc)\n",
    "    \n",
    "    return sum(val_acc) / len(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "portable-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "    \n",
    "#     print(f'Epoch: {epoch}')\n",
    "        \n",
    "#     train_model(model, graph_train_loader, preprocess, device)\n",
    "#     val_acc = validate_model(model, graph_val_loader, preprocess, device)\n",
    "#     train_acc = validate_model(model, graph_train_loader, preprocess, device)\n",
    "    \n",
    "#     print(f'Train Acc: {train_acc}, Val Acc: {val_acc}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "furnished-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_test_loader = DataLoader(graph_test_dataset, batch_size=batch_size)\n",
    "# test_acc = validate_model(model, graph_test_loader, preprocess, device)\n",
    "# test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-reach",
   "metadata": {},
   "source": [
    "## Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "laughing-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gvae(model, train_dataset_loader, optimiser, preprocessor, device):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_dataset_loader):\n",
    "        optimiser.zero_grad()\n",
    "        batch = preprocessor(batch.to(device))\n",
    "        edge_probs, z, mean, log_std = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        loss, _, _ = model.loss_function(mean, log_std, edge_probs)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "def validate_gvae(model, val_dataset_loader, preprocessor, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        val_elbos = []\n",
    "        log_lik = []\n",
    "        kl_losses = []\n",
    "\n",
    "        for batch in tqdm.tqdm(val_dataset_loader):\n",
    "            batch = preprocessor(batch.to(device))\n",
    "            edge_probs, z, mean, log_std = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            elbo, log_prob, kl_loss = model.loss_function(mean, log_std, edge_probs)\n",
    "            val_elbos.append(elbo)\n",
    "            log_lik.append(log_prob)\n",
    "            kl_losses.append(kl_loss)\n",
    "            \n",
    "    total_elbo = sum(val_elbos) / len(val_elbos)\n",
    "    total_log_lik = sum(log_lik) / len(log_lik)\n",
    "    total_kl = sum(kl_losses) / len(kl_losses)\n",
    "    \n",
    "    return total_elbo, total_log_lik, total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "approved-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "\n",
    "train_frac = 0.8\n",
    "train_idx = int(len(graph_train_dataset) * train_frac)\n",
    "train_split = graph_train_dataset[:train_idx]\n",
    "val_split = graph_train_dataset[train_idx:]\n",
    "\n",
    "batch_size = 512\n",
    "graph_train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=False)\n",
    "graph_val_loader = DataLoader(val_split, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "martial-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch_geometric.nn import DeepGCNLayer\n",
    "\n",
    "\n",
    "LOG_CONST = 1e-15\n",
    "\n",
    "\n",
    "class SplineConvUnit(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, edge_dim, dropout=0, latent_encoder=False):\n",
    "        super(SplineConvUnit, self).__init__()\n",
    "        self.edge_dim = edge_dim\n",
    "        self.dropout = dropout\n",
    "        self.conv = gnn.SplineConv(input_dim, output_dim, dim=edge_dim, kernel_size=5)\n",
    "        self.latent_encoder = latent_encoder\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        \n",
    "        if self.latent_encoder:\n",
    "            return x\n",
    "        \n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class GCNConvUnit(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, edge_dim, dropout=0, latent_encoder=False):\n",
    "        super(GCNConvUnit, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv = gnn.GCNConv(input_dim, output_dim)\n",
    "        self.latent_encoder = latent_encoder\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        x = self.conv(x, edge_index)\n",
    "        \n",
    "        if self.latent_encoder:\n",
    "            return x\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_class, input_dim, hidden_dim1, hidden_dim2, latent_dim, edge_dim=None, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = conv_class(input_dim, hidden_dim1, edge_dim, dropout)\n",
    "        self.conv2 = conv_class(hidden_dim1, hidden_dim2, edge_dim, dropout)\n",
    "        \n",
    "        kwargs = {\n",
    "            'input_dim': hidden_dim2,\n",
    "            'output_dim': latent_dim,\n",
    "            'edge_dim': edge_dim,\n",
    "            'latent_encoder': True,\n",
    "        }\n",
    "        \n",
    "        if conv_class == SplineConvUnit:\n",
    "            kwargs['edge_dim'] = edge_dim\n",
    "        \n",
    "        self.mean = conv_class(**kwargs)\n",
    "        self.log_std = conv_class(**kwargs)\n",
    "        self.edge_dim = edge_dim\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def get_output_dim(self):\n",
    "        return self.output_dim\n",
    "    \n",
    "    def get_edge_dim(self):\n",
    "        return self.edge_dim\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> [Tensor, Tensor]:\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        mean = self.mean(x, edge_index, edge_attr)\n",
    "        log_std = self.log_std(x, edge_index, edge_attr)\n",
    "        return mean, log_std\n",
    "    \n",
    "\n",
    "class GraphVAE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GraphVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> [Tensor, Tensor]:\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "    \n",
    "    def decode(self, z: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        return self.decoder(z, edge_index)\n",
    "    \n",
    "    def reparametrise(self, mean: Tensor, log_std: Tensor) -> Tensor:\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Note: Just 1 MC particle\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps * std\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> [Tensor, Tensor, Tensor, Tensor]:\n",
    "        res = self.encode(x, edge_index, edge_attr)\n",
    "        mean, log_std = self.encode(x, edge_index, edge_attr)\n",
    "        z = self.reparametrise(mean, log_std)\n",
    "        return self.decode(z, edge_index), z, mean, log_std\n",
    "\n",
    "    def generate(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)[0]\n",
    "    \n",
    "    def loss_function(self, mean: Tensor, log_std: Tensor, pos_edge_probs: Tensor) -> Tensor:\n",
    "        pos_log_prob = -torch.log(pos_edge_probs + LOG_CONST).mean()\n",
    "        # neg_log_prob = -torch.log(1 - neg_edge_probs + LOG_CONST).mean()\n",
    "        log_prob = pos_log_prob  # + neg_log_prob\n",
    "        \n",
    "        kl_loss = torch.sum(1 + 2 * log_std - mean ** 2 - log_std.exp() ** 2, dim=1)\n",
    "        kl_loss = -0.5 * torch.mean(kl_loss)\n",
    "        \n",
    "        loss = log_prob + kl_loss\n",
    "        \n",
    "        return loss, log_prob, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "abroad-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = gnn_T.Compose([\n",
    "    gnn_T.TargetIndegree()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "dominican-prayer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): GCNConvUnit(\n",
       "      (conv): GCNConv(1, 32)\n",
       "    )\n",
       "    (conv2): GCNConvUnit(\n",
       "      (conv): GCNConv(32, 16)\n",
       "    )\n",
       "    (mean): GCNConvUnit(\n",
       "      (conv): GCNConv(16, 10)\n",
       "    )\n",
       "    (log_std): GCNConvUnit(\n",
       "      (conv): GCNConv(16, 10)\n",
       "    )\n",
       "  )\n",
       "  (decoder): InnerProductDecoder()\n",
       ")"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import InnerProductDecoder\n",
    "\n",
    "input_dim = 1\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "edge_dim = 1\n",
    "latent_dim = 10\n",
    "dropout = 0\n",
    "\n",
    "conv_class = GCNConvUnit\n",
    "encoder = Encoder(conv_class, input_dim, hidden1, hidden2, latent_dim, edge_dim, dropout).to(device)\n",
    "decoder = InnerProductDecoder().to(device)\n",
    "gvae = GraphVAE(encoder, decoder).to(device)\n",
    "\n",
    "gvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "interim-custom",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape\n",
      "Batch(batch=[225], edge_attr=[4043, 1], edge_index=[2, 4043], pos=[225, 2], x=[225, 1], y=[3])\n",
      "torch.Size([225, 1]) torch.Size([2, 4043]) torch.Size([4043, 1])\n",
      "\n",
      "Output shape\n",
      "torch.Size([4043]) torch.Size([225, 10]) torch.Size([225, 10]) torch.Size([225, 10])\n",
      "\n",
      "(tensor(1.5489, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.5436, device='cuda:0', grad_fn=<NegBackward>), tensor(0.0053, device='cuda:0', grad_fn=<MulBackward0>))\n"
     ]
    }
   ],
   "source": [
    "graph_trial_loader = DataLoader(graph_train_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "for x in graph_trial_loader:\n",
    "    break\n",
    "\n",
    "gvae.eval()\n",
    "    \n",
    "_data = preprocess(x.to(device))\n",
    "\n",
    "print('Data shape')\n",
    "print(_data)\n",
    "print(_data.x.shape, _data.edge_index.shape, _data.edge_attr.shape)\n",
    "print()\n",
    "\n",
    "edge_probs, z, mean, log_std = gvae(_data.x, _data.edge_index, _data.edge_attr)\n",
    "print('Output shape')\n",
    "print(edge_probs.shape, z.shape, mean.shape, log_std.shape)\n",
    "print()\n",
    "\n",
    "print(gvae.loss_function(mean, log_std, edge_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "buried-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/94 [00:00<00:07, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:07<00:00, 13.24it/s]\n",
      "  8%|▊         | 2/24 [00:00<00:01, 16.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 15.80it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:05, 17.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 17.76it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:06, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Elbo: 1.3891448974609375, Val Elbo: 1.3890862464904785\n",
      "Train Log Lik: 1.3729982376098633, Val Log Lik: 1.3730144500732422\n",
      "Train KL: 0.016146957874298096, Val KL: 0.016071707010269165\n",
      "\n",
      "Epoch: 1\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:07<00:00, 13.24it/s]\n",
      "  8%|▊         | 2/24 [00:00<00:01, 16.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 18.14it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:05, 18.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 18.09it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:06, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Elbo: 1.360724687576294, Val Elbo: 1.361077070236206\n",
      "Train Log Lik: 1.327467441558838, Val Log Lik: 1.3279004096984863\n",
      "Train KL: 0.03325744345784187, Val KL: 0.03317663446068764\n",
      "\n",
      "Epoch: 2\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:07<00:00, 13.31it/s]\n",
      "  8%|▊         | 2/24 [00:00<00:01, 16.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 18.10it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:04, 18.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 17.21it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:06, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Elbo: 1.3393208980560303, Val Elbo: 1.3388365507125854\n",
      "Train Log Lik: 1.284861445426941, Val Log Lik: 1.2844533920288086\n",
      "Train KL: 0.054458584636449814, Val KL: 0.05438306927680969\n",
      "\n",
      "Epoch: 3\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:07<00:00, 13.27it/s]\n",
      "  8%|▊         | 2/24 [00:00<00:01, 16.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 18.18it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:05, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 18.04it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:06, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Elbo: 1.3318439722061157, Val Elbo: 1.3324756622314453\n",
      "Train Log Lik: 1.2642377614974976, Val Log Lik: 1.2649564743041992\n",
      "Train KL: 0.067606121301651, Val KL: 0.06751897931098938\n",
      "\n",
      "Epoch: 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:07<00:00, 13.32it/s]\n",
      "  8%|▊         | 2/24 [00:00<00:01, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 18.05it/s]\n",
      "  2%|▏         | 2/94 [00:00<00:05, 18.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:05<00:00, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Elbo: 1.3269635438919067, Val Elbo: 1.3278532028198242\n",
      "Train Log Lik: 1.2514976263046265, Val Log Lik: 1.2524890899658203\n",
      "Train KL: 0.07546579092741013, Val KL: 0.07536416500806808\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimiser_gvae = torch.optim.Adam(gvae.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    print(f'Epoch: {epoch}')\n",
    "    print('Training')\n",
    "    train_gvae(gvae, graph_train_loader, optimiser_gvae, preprocess, device)\n",
    "    print('Validation - Val set')\n",
    "    mean_val_elbo, mean_val_log_lik, mean_val_total_kl = validate_gvae(gvae, graph_val_loader, preprocess, device)\n",
    "    print('Validation - Train set')\n",
    "    mean_train_elbo, mean_train_log_lik, mean_train_total_kl = validate_gvae(gvae, graph_train_loader, preprocess, device)\n",
    "    \n",
    "    print(f'Train Elbo: {mean_train_elbo}, Val Elbo: {mean_val_elbo}')\n",
    "    print(f'Train Log Lik: {mean_train_log_lik}, Val Log Lik: {mean_val_log_lik}')\n",
    "    print(f'Train KL: {mean_train_total_kl}, Val KL: {mean_val_total_kl}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-cutting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
